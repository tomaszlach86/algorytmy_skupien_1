## ğŸ§© CzÄ™Å›Ä‡ C â€“ Analiza parametrÃ³w i wnioski

### 1. Zmiany centroidÃ³w i przypisaÅ„ punktÃ³w przy rÃ³Å¼nych wartoÅ›ciach k
- Dla **k = 2** punkty grupujÄ… siÄ™ w dwa wyraÅºne zbiory â€“ jeden skupiony w okolicy maÅ‚ych wartoÅ›ci wspÃ³Å‚rzÄ™dnych (lewa dolna czÄ™Å›Ä‡ wykresu), a drugi w prawej gÃ³rnej czÄ™Å›ci.
- Dla **k = 3** algorytm rozdziela jednÄ… z istniejÄ…cych grup na dwa mniejsze podzbiory. Centroidy przesuwajÄ… siÄ™, by lepiej odzwierciedliÄ‡ lokalne skupienia punktÃ³w.
- Wraz ze wzrostem liczby klastrÃ³w centroidy stajÄ… siÄ™ bardziej "lokalne", a granice miÄ™dzy klastrami ostrzejsze.

---

### 2. Rozmyta przynaleÅ¼noÅ›Ä‡ (FCM) vs twarde przypisanie (K-means)
- W **K-means** kaÅ¼dy punkt naleÅ¼y **tylko do jednego** klastra â€“ przypisanie jest binarne (twarde).
- W **Fuzzy C-Means** (FCM) punkty majÄ… **stopieÅ„ przynaleÅ¼noÅ›ci** do kaÅ¼dego klastra, co pozwala lepiej oddaÄ‡ niejednoznacznoÅ›Ä‡ danych.
- Punkty poÅ‚oÅ¼one â€na granicyâ€ miÄ™dzy klastrami majÄ… w FCM przynaleÅ¼noÅ›ci zbliÅ¼one do 0.5 / 0.5, podczas gdy w K-means muszÄ… zostaÄ‡ przypisane arbitralnie do jednego klastra.
- DziÄ™ki temu FCM jest bardziej realistyczny w modelowaniu danych, gdzie granice miÄ™dzy grupami nie sÄ… ostre.

---

### 3. WpÅ‚yw punktÃ³w odstajÄ…cych (szum w danych)
- Po dodaniu punktu odstajÄ…cego (np. **P10 = [18.5, 11.5]**) algorytm **K-means** reaguje silnie â€“ centroid jednego klastra przesuwa siÄ™ w stronÄ™ punktu odstajÄ…cego.
- MoÅ¼e to spowodowaÄ‡, Å¼e pozostaÅ‚e punkty zostanÄ… niepoprawnie przypisane lub klaster bÄ™dzie reprezentowany tylko przez punkt odstajÄ…cy.
- W **Fuzzy C-Means** przynaleÅ¼noÅ›Ä‡ punktu odstajÄ…cego rozmywa siÄ™ (np. 0.6 / 0.4), co ogranicza jego wpÅ‚yw na centroidy pozostaÅ‚ych grup.
- DziÄ™ki temu FCM lepiej radzi sobie z danymi zawierajÄ…cymi szum i pojedyncze odlegÅ‚e obserwacje.

---

### 4. ElastycznoÅ›Ä‡ algorytmÃ³w wobec punktÃ³w odstajÄ…cych
- **Fuzzy C-Means** jest bardziej elastyczny, poniewaÅ¼ nie wymusza jednoznacznego przypisania punktu do klastra.
- Daje moÅ¼liwoÅ›Ä‡ czÄ™Å›ciowej przynaleÅ¼noÅ›ci do wielu klastrÃ³w, co ogranicza wpÅ‚yw pojedynczych ekstremalnych obserwacji.
- **K-means** jest bardziej wraÅ¼liwy, poniewaÅ¼ centroid opiera siÄ™ na Å›redniej, ktÃ³ra silnie reaguje na wartoÅ›ci odstajÄ…ce.

---

### 5. WpÅ‚yw liczby klastrÃ³w na wyniki
- Dla maÅ‚ej liczby klastrÃ³w (`k=2`) algorytmy tworzÄ… ogÃ³lne, szerokie grupy, ktÃ³re Å‚Ä…czÄ… punkty o rÃ³Å¼nej charakterystyce.
- Dla wiÄ™kszej liczby klastrÃ³w (`k=3` i wiÄ™cej) pojawia siÄ™ dokÅ‚adniejszy, ale bardziej szczegÃ³Å‚owy podziaÅ‚ â€” niekiedy zbyt drobny (tzw. **overfitting**).
- W praktyce optymalnÄ… liczbÄ™ klastrÃ³w warto dobieraÄ‡ empirycznie, np. metodÄ… **Å‚okcia** lub **silhouette score**.

---

### ğŸ’¡ Podsumowanie
- **K-means** jest szybki, prosty i intuicyjny, ale mniej odporny na szum i wymaga z gÃ³ry okreÅ›lenia liczby klastrÃ³w.
- **Fuzzy C-Means** jest bardziej elastyczny, pozwala na analizÄ™ niejednoznacznych przypadkÃ³w i lepiej radzi sobie z danymi rozmytymi lub zawierajÄ…cymi odstajÄ…ce punkty.
- W przypadku danych z wyraÅºnymi granicami â€“ K-means daje wystarczajÄ…co dobre wyniki.
- W przypadku danych niejednoznacznych lub z szumem â€“ Fuzzy C-Means pozwala uzyskaÄ‡ bardziej realistyczne i stabilne grupowanie.
